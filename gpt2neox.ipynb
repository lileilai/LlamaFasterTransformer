{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfde86c3-edc7-45f8-bd38-363ae13264ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b0aad9-d7ba-4340-aaee-6e1493e90788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded on device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"/data1/lileilai/gpt_neox_20b/\"\n",
    "model_id = \"/data1/lileilai/opt-6.7b/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,)\n",
    "# we use device_map auto to automatically place all shards on the GPU to save CPU memory\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "print(f\"model is loaded on device {model.device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe297ac-5f6f-4944-ba9e-37ae869ed935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c02706c-4cb8-4235-8021-d665db0eeb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = dict(max_new_tokens=100, do_sample=False, use_cache=True)\n",
    "# tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6586e213-f979-4c85-b042-aef2cee29029",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c201654-e6fb-4814-b0bc-77478defacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(100,10)\n",
    "    def forward(self, hidden_state):\n",
    "        return self.fc1(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491902c9-05d3-45a3-bb3a-9f284051bcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b246a8e9-5636-47fb-b6ff-cdc1489d4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = ['In the last couple of days, a',\n",
    " 'The New Jersey Department of Transportation is aware',\n",
    " 'The New York Giants have a new head',\n",
    " 'The New York Times has published its annual',\n",
    " 'In a move that will likely make it',\n",
    " \"The New York Giants' offensive linemen\",\n",
    " 'The Canadian Press has unanimously condemned the new',\n",
    " 'The first time I saw the movie,'\n",
    "]\n",
    "batch_size = 8\n",
    "inputs = input_sentences[: 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d00247-2cc4-4f9c-bc78-32dfdd87b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    input_tokens = tokenizer.batch_encode_plus(input_sentences, return_tensors=\"pt\", padding=True)\n",
    "    for t in input_tokens:\n",
    "        if torch.is_tensor(input_tokens[t]):\n",
    "            input_tokens[t] = input_tokens[t].to(\"cuda:0\")\n",
    "\n",
    "    outputs = model.generate(**input_tokens, **generate_kwargs)\n",
    "\n",
    "    input_tokens_lengths = [x.shape[0] for x in input_tokens.input_ids]\n",
    "    output_tokens_lengths = [x.shape[0] for x in outputs]\n",
    "\n",
    "    total_new_tokens = [o - i for i, o in zip(input_tokens_lengths, output_tokens_lengths)]\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return zip(inputs, outputs, total_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1231b7df-a166-4e6c-8198-212f7b83758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import os\n",
    "local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "world_size = torch.cuda.device_count()\n",
    "\n",
    "rank = local_rank\n",
    "def print_rank0(*msg):\n",
    "    if rank != 0:\n",
    "        return\n",
    "    print(*msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b370cbb-5b98-4227-9820-34cd18e0480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Running generate\n",
      "*** Running benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.21852560043335\n",
      "\n",
      "*** Performance stats:\n",
      "Throughput per token including tokenize: 4.02 msecs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Running generate\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print_rank0(\"*** Running benchmark\")\n",
    "# warm up\n",
    "for i in range(1):\n",
    "    _ = generate()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# benchmark\n",
    "t0 = time.time()\n",
    "cycles = 5\n",
    "total_new_tokens_generated = 0\n",
    "for i in range(cycles):\n",
    "    generated = generate()\n",
    "    total_new_tokens_generated += sum(new_tokens for _, _, new_tokens in generated)\n",
    "torch.cuda.synchronize()\n",
    "throughput = (time.time() - t0) / (total_new_tokens_generated)\n",
    "print((time.time() - t0 ) / cycles )\n",
    "print_rank0(\n",
    "        f\"\"\"\n",
    "*** Performance stats:\n",
    "Throughput per token including tokenize: {throughput*1000:.2f} msecs\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fa9d42-6866-44f1-8c47-92f946fac550",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_data \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43moutputs\u001b[49m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "output_data = [tokenizer.decode(d) for d in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c48eedeb-43b7-4db0-9531-6ebb415b34ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the last couple of days, a lot of people have been asking me about the new book,',\n",
       " 'The New Jersey Department of Transportation is aware of the issue and is working with the contractor to resolve it',\n",
       " 'The New York Giants have a new head coach, and he’s a former NFL quarterback.\\n',\n",
       " 'The New York Times has published its annual list of the most influential people in the world, and the',\n",
       " 'In a move that will likely make it easier for the company to raise money, the company said it',\n",
       " \"The New York Giants' offensive linemen are a big reason why the team has been so successful in\",\n",
       " 'The Canadian Press has unanimously condemned the new law, saying it is “unacceptable” and “un',\n",
       " 'The first time I saw the movie, I was in the theater and I was like, \"Oh']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "664aa996-060f-49a8-acaa-0cd105479bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.801179575920105"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(consume_time) / len(consume_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a34f1ee-5cab-4739-a984-7d7b9668982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd5d9755-dad7-4ba7-92f4-6061a9c6662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d + ((1,2),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9048c8-7f67-4b03-8c8b-cdea7713d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d + ((3, 4),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcdb605b-6fb4-4888-ab10-127a89e8a120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2), (3, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce2734-eeb3-4718-b096-0a2a133c43b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
